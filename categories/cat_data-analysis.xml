<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Thejasvi Beleyur (Posts about data analysis)</title><link>https://example.com/</link><description></description><atom:link href="https://example.com/categories/cat_data-analysis.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:thejasvib@gmail.com"&gt;Thejasvi Beleyur&lt;/a&gt; </copyright><lastBuildDate>Sat, 11 Apr 2020 06:55:05 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Universal data formats</title><link>https://example.com/blog/universal_data_formats/</link><dc:creator>Thejasvi Beleyur</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm still trying to get through an old but (I think) cool analysis for a manuscript 
that's over five years in the making now. In general, the code in the manuscript
itself reflects the need for mutiple coding platforms and how each of  them 
bring its own superpowers with it. All of the image analysis results in the project
has been done in MATLAB. The stats and analyses have been beautifully documented
in R with Markdown notebooks. A collaborator did some additional analyses in 
MATLAB recently and sent over some new results. &lt;/p&gt;
&lt;p&gt;It would have all been fine, and I probably  wouldn't have even written the post
if I was still using the same laptop I had when the project started. Now, however, 
even though the laptop is  the same, it's gone through a couple of OS changes 
and currenly has  R and Python installed in it -  same cover different contents. Getting some .mat files and having to open
them to analyse them is not impossible. I'm very thankful for cross-language packages like 
&lt;a href="https://cran.r-project.org/web/packages/R.matlab/index.html"&gt;R.matlab&lt;/a&gt; and the inbuilt &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html"&gt;scipy.io.loadmat&lt;/a&gt; for existing, and am able to load the data without problem. &lt;/p&gt;
&lt;p&gt;However, even the act of having to find a specialised package to load a dataset
saved in a platform-specific format made me re-think how I would save my data in the
future. It's not a computing platform based platform, it seems only natural to 
end the day by saving the results  into  one .Rda or .pkl file. The issue is&lt;br&gt;
when these same files have to be read by  someone else who's not invested
in the same platform. Simple question, what if I'd decided to send a collaborator
a .Rda/.pkl file, and they use a completely different computing platform which
is called say... Kidneybeans? Kidneybeans is an established platform in the 
field of MagicMaking and has a small but established community of researchers
using it. Should your collaborator bend to the pressure of a larger established 
community and spend 45 minutes of their time just to load and re-format the data?
Not fair right?&lt;/p&gt;
&lt;p&gt;The solution to overcoming cross-language barriers is of course to use 'standard' formats (csv, json, hdf5). 
This has been suggested multiple times &lt;a href="https://example.com/blog/universal_data_formats/"&gt;ref1,ref2&lt;/a&gt; and it's only beginning to 
dawn on me. It does require some effort, to plan and re-organise all the data
into standard formats instead of being able to 'naturally' dump that list, 
data frame and 3-channel image array into one file. I guess, the main advantage of 
saving stuff into universal formats is the data will still remain accessible 
to future collaborators using &lt;em&gt;any&lt;/em&gt; computational platform, and of course 
the most likely future collaborator as always is &lt;strong&gt;futureyou&lt;/strong&gt;!&lt;/p&gt;&lt;/div&gt;</description><guid>https://example.com/blog/universal_data_formats/</guid><pubDate>Thu, 09 Apr 2020 07:27:00 GMT</pubDate></item><item><title>I need to finish this analysis for a manuscript that's *X* years old (...gasp)</title><link>https://example.com/blog/keeping_track_of_old/</link><dc:creator>Thejasvi Beleyur</dc:creator><description>&lt;div&gt;&lt;p&gt;With the social distancing measures implemented thanks to the COVID19 pandemic, I saw a whole wave of memes about academics thinking they'd get more productive. 
Now, that there're no more unnecessary meetings, teaching, or random interruptions from the workplace, it'd be great to &lt;em&gt;finally&lt;/em&gt; get back to doing &lt;strong&gt;SCAAAAAAINCE&lt;/strong&gt;. 
I myself, wasn't quite sure, as an end phase PhD student working at a mainly research institute, I can't quite complain about any of these things, although I will say I am beginning to enjoy the luxury and convenience of waking up, getting ready, and heading to the desk a few metres away. &lt;/p&gt;
&lt;p&gt;One of the things I've been trying to get done is make some new plots and add some data analyses from a project I was part of back in 2015. Yes, the project is now almost six years old. The manuscript has gone through one rejection, been revamped a bit and now it's soon to be submitted at its second journal. I actually began working in full steam on the analysis about a month ago, and made a decent amount of progress getting back to R and writing up  a new Markdown notebook to document the
analysis as I was doing it, and that's when I began to realise how !@#$'ing hard it is to keep track of experiments, data and analyses that happened anything more than a few months ago. &lt;/p&gt;
&lt;p&gt;Old code, that's not tested or documented well can be a nightmare. Old experimental analyses that're semi-documented and with a bunch of intermediate files lying aroud everywhere - that's just torture. And yes, I will admit that this always happens whenever I'm working with my (not-so) favourite collaborator, past me. Past me has a habit of coming up  with cool ideas, and then putting them into the same Rmd notebook as the final figures for journal submission. My other favourite is past me does this irritating thing of having done a bunch of very cool stuff about 75% of the way, but then forgetting to say what else needs to be done in case someone'd like to continue the work forward. The burden of having to figure it all out afresh each time, of course, means there's a growing sense of reluctance each time.&lt;/p&gt;
&lt;p&gt;This is the point where I began to think about what could be done to improve the situation. I have heard from my colleagues who say they always have a fixed folder structure for instance, or that they keep all the 'old' stuff in one folder and the manuscript-worthy stuff in another. I guess there is a lot of room for  personal choice. However, the one thing with personal choice is that it means there are some real bad ideas that're not propagated, but even worse, there're some great ideas that don't reach out  too! One of  the things I picked  up from following conventions (eg. coding and documentation conventions) is the power and discipline they bring to the way you  write code. I have this nagging feeling there must be a field of research or industry where a nice set of protocols must have been formulated about  how to organise the code, raw data and processed data properly. I'd now venture to say, it &lt;strong&gt;must&lt;/strong&gt; be out there...time to start looking properly!!&lt;/p&gt;&lt;/div&gt;</description><guid>https://example.com/blog/keeping_track_of_old/</guid><pubDate>Wed, 08 Apr 2020 06:26:00 GMT</pubDate></item></channel></rss>